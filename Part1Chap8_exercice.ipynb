{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71cc5a4",
   "metadata": {},
   "source": [
    "# Chapter 8 : Using convolutions to generalize\n",
    "\n",
    "### 1. Change our model to use a 5 × 5 kernel with kernel_size=5 passed to the nn.Conv2d constructor.\n",
    "* What impact does this change have on the number of parameters in the model?\n",
    "* Does the change improve or degrade overfitting?\n",
    "* Read https://pytorch.org/docs/stable/nn.html#conv2d.\n",
    "* Can you describe what kernel_size=(1,3) will do?\n",
    "* How does the model behave with such a kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a9805",
   "metadata": {},
   "source": [
    "In this chapter, we have seen how to make networks as a submodule of the `nn.Module` of Pytorch and how to use convolution and pooling layers to exploit locality, featuring translation invariance and diminishing the size of the feature images. \\\n",
    "Also, we have seen how to create deeper networks and especially how to prevent *vanishing gradient problem* by using tricks like *Batch Normalization* or by using a *skip-connection* for even deeper networks.\n",
    "\n",
    "We won't use those models here because we are dealing whith small images (3x32x32) and then it is overkill, as the authors say : \\\n",
    "\"*All this shouldn’t encourage us to seek depth on a dataset of 32 × 32 images, but it clearly demonstrates how this can be achieved on more challenging datasets like Image- Net. It also provides the key elements for understanding existing implementations for models like ResNet, for instance, in torchvision.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3688f",
   "metadata": {},
   "source": [
    "In this notebook we will use the model created in the chapter 8 of the book. It was write as an `nn.Module` of Pytorch and it is composed of two hidden layers. \\\n",
    "We recalling it below and re-creating our dataset :\n",
    "\n",
    "<img style=\"float: center;\" src='data/img_NET.jpg' width=\"200\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "\n",
    "\n",
    "Note : the models have been already trained, all parameters were saved as a `.pt` file in the `/data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "88274908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6b8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=False, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "]))\n",
    "\n",
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75d955f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameter :  38386\n",
      "Number of parameters in each layer :  [864, 32, 4608, 16, 32768, 32, 64, 2]\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,  n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8*8*n_chans1//2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8*8*self.n_chans1//2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = Net()\n",
    "numel_list = [p.numel() for p in model.parameters()] \n",
    "print('Total number of parameter : ', sum(numel_list))\n",
    "print(\"Number of parameters in each layer : \", numel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbfb3419",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameter :  48114\n",
      "Number of parameters in each layer :  [2400, 32, 12800, 16, 32768, 32, 64, 2]\n"
     ]
    }
   ],
   "source": [
    "### Changing kernel_size\n",
    "class Net_newkernel(nn.Module):\n",
    "    def __init__(self,  n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(8*8*n_chans1//2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8*8*self.n_chans1//2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model_kernel5 = Net_newkernel()\n",
    "numel_list = [p.numel() for p in model_kernel5.parameters()] \n",
    "print('Total number of parameter : ', sum(numel_list))\n",
    "print(\"Number of parameters in each layer : \", numel_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce224ce1",
   "metadata": {},
   "source": [
    "Without changing the desired output channel size and by selecting a convolution kernel of 5x5, the total number of parameters of our model increases by 25%. \n",
    "Note : we had to change the padding to be consistant in terms of output sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72d87857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)  # <1>\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            outputs = model(imgs) \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "            \n",
    "def validate(model, train_loader, val_loader):\n",
    "    accuracy_dict = {}\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accuracy_dict[name] = correct / total\n",
    "    return accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac22bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving / Loading parameters\n",
    "def save_load(save_model, load_model, path, name, model_name):\n",
    "    if save_model:\n",
    "        torch.save(model_name.state_dict(), path + name)\n",
    "        print(f\"Parameters of the model {model_name} have been successfully saved\")\n",
    "        return\n",
    "\n",
    "    if load_model:\n",
    "        model_name = model_name().to(device=device)\n",
    "        model_name.load_state_dict(torch.load(path + name, map_location=device))\n",
    "        print(f\"Parameters have been successfully loaded in {model_name}\")\n",
    "        return model_name\n",
    "    \n",
    "    if load_model == False and save_model == False:\n",
    "        raise Exception(\"You have to specify whether you want to save or load a model.\")\n",
    "    \n",
    "    if load_model and save_model:\n",
    "        raise Exception(\"It can be misleading to save and load the parameters at the same time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dadf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "40ab2621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-27 13:12:33.428234 Epoch 1, Training loss 0.5566195162238589\n",
      "2021-08-27 13:14:52.495401 Epoch 10, Training loss 0.3142995872315328\n",
      "2021-08-27 13:17:22.254285 Epoch 20, Training loss 0.26879982953997933\n",
      "2021-08-27 13:19:42.788088 Epoch 30, Training loss 0.23711505328204222\n",
      "2021-08-27 13:22:04.542019 Epoch 40, Training loss 0.20966298901920866\n",
      "2021-08-27 13:24:24.719137 Epoch 50, Training loss 0.18511778068770268\n",
      "2021-08-27 13:26:45.803298 Epoch 60, Training loss 0.16246780475518505\n",
      "2021-08-27 13:29:03.531775 Epoch 70, Training loss 0.140480528449177\n",
      "2021-08-27 13:31:36.007464 Epoch 80, Training loss 0.11949570617003805\n",
      "2021-08-27 13:34:06.645789 Epoch 90, Training loss 0.1000917593408732\n",
      "2021-08-27 13:36:28.340460 Epoch 100, Training loss 0.082368084816796\n"
     ]
    }
   ],
   "source": [
    "### Kernel size = 3x3\n",
    "model_kernel3 = Net().to(device=device)\n",
    "optimizer33 = optim.SGD(model_kernel3.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer33,\n",
    "    model = model_kernel3,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "645eaadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thierry/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459044803/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-26 08:04:59.188686 Epoch 1, Training loss 0.5278796969325679\n",
      "2021-08-26 08:08:41.699998 Epoch 10, Training loss 0.3009436071678332\n",
      "2021-08-26 08:12:47.353033 Epoch 20, Training loss 0.24531514562998608\n",
      "2021-08-26 08:16:58.558810 Epoch 30, Training loss 0.20092373787407664\n",
      "2021-08-26 08:21:03.786149 Epoch 40, Training loss 0.16242096444983392\n",
      "2021-08-26 08:25:29.606520 Epoch 50, Training loss 0.12756616849998\n",
      "2021-08-26 08:29:37.431415 Epoch 60, Training loss 0.09597909934581465\n",
      "2021-08-26 08:33:41.640914 Epoch 70, Training loss 0.06892329480522758\n",
      "2021-08-26 08:38:21.695474 Epoch 80, Training loss 0.04768813655349859\n",
      "2021-08-26 08:42:38.752955 Epoch 90, Training loss 0.03287433337859193\n",
      "2021-08-26 08:47:05.050042 Epoch 100, Training loss 0.022978694857376965\n"
     ]
    }
   ],
   "source": [
    "### Kernel size = 5x5\n",
    "model_kernel5 = Net_newkernel().to(device=device)\n",
    "optimizer55 = optim.SGD(model_kernel5.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer55,\n",
    "    model = model_kernel5,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23eca994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.99\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "_ = validate(model_kernel5, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21212eee",
   "metadata": {},
   "source": [
    "So the train score in the case of `kernel_size=5` is higher than the one with `kernel_size=3` and the validation accuracies seem to be the same. So, one can deduce that those changes improve overfitting : the train dataset is (almost) perfectly learnt, but it doesn't improve the capacity of the model to generalize through unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ce34cbd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape with square kernel and padding = 1 torch.Size([1, 32, 32, 32])\n",
      "Output shape with non-square kernel and padding = 0 torch.Size([1, 32, 32, 30])\n",
      "Output shape with non-square kernel and padding = 1 torch.Size([1, 32, 34, 32])\n",
      "Output shape with non-square kernel and padding = (0,1) torch.Size([1, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "conv33 = nn.Conv2d(3, 32, kernel_size=(3,3), padding=1)\n",
    "conv13_padding0 = nn.Conv2d(3, 32, kernel_size=(1,3), padding=0)\n",
    "conv13_padding1 = nn.Conv2d(3, 32, kernel_size=(1,3), padding=1)\n",
    "conv13_padding01 = nn.Conv2d(3, 32, kernel_size=(1,3), padding=(0,1))\n",
    "img, _ = cifar2[100]\n",
    "    \n",
    "output33 = conv33(img.unsqueeze(0))\n",
    "output13_padding0 = conv13_padding0(img.unsqueeze(0))\n",
    "output13_padding1 = conv13_padding1(img.unsqueeze(0))\n",
    "output13_padding01 = conv13_padding01(img.unsqueeze(0))\n",
    "\n",
    "print(\"Output shape with square kernel and padding = 1\", output33.shape)\n",
    "print(\"Output shape with non-square kernel and padding = 0\", output13_padding0.shape)\n",
    "print(\"Output shape with non-square kernel and padding = 1\", output13_padding1.shape)\n",
    "print(\"Output shape with non-square kernel and padding = (0,1)\", output13_padding01.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535921ac",
   "metadata": {},
   "source": [
    "Using a non-square kernel size as `kernel_size=(1,3)` will focus on horizontal detection since there is 3 times more of horizontal than vertical weights. Also, since it's a non-square kernel, we have to set the padding with a tuple `padding = (0,1)` to keep the outputs with the same shape compared to the inputs.\n",
    "\n",
    "See also : https://stackoverflow.com/questions/59946176/non-squared-convolution-kernel-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d37995b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_newkernel(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kernel5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6af3704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Changing kernel_size\n",
    "class Net_newkernel13(nn.Module):\n",
    "    def __init__(self,  n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=(1,3), padding=(0,1))\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=(1,3), padding=(0,1))\n",
    "        self.fc1 = nn.Linear(8*8*n_chans1//2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8*8*self.n_chans1//2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7146b823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-27 01:08:38.589325 Epoch 1, Training loss 0.5460081045414992\n",
      "2021-08-27 01:10:14.091815 Epoch 10, Training loss 0.34842085249864374\n",
      "2021-08-27 01:11:57.321762 Epoch 20, Training loss 0.3048952476234193\n",
      "2021-08-27 01:13:40.956657 Epoch 30, Training loss 0.2738129356104857\n",
      "2021-08-27 01:15:33.337801 Epoch 40, Training loss 0.24778122777581973\n",
      "2021-08-27 01:17:20.499185 Epoch 50, Training loss 0.22464247394329423\n",
      "2021-08-27 01:19:05.138778 Epoch 60, Training loss 0.20445078108341072\n",
      "2021-08-27 01:20:55.802694 Epoch 70, Training loss 0.18624177722224763\n",
      "2021-08-27 01:22:38.342078 Epoch 80, Training loss 0.16885942699992731\n",
      "2021-08-27 01:24:07.769352 Epoch 90, Training loss 0.15164617847674972\n",
      "2021-08-27 01:25:34.456400 Epoch 100, Training loss 0.13446136367074243\n"
     ]
    }
   ],
   "source": [
    "model_kernel13 = Net_newkernel13().to(device=device)\n",
    "optimizer13 = optim.SGD(model_kernel13.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer13,\n",
    "    model = model_kernel13,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b1705d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.94\n",
      "Accuracy val: 0.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 0.9427, 'val': 0.89}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model_kernel13, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008800db",
   "metadata": {},
   "source": [
    "Well we can see that the train score decreased while the validation score is still the same. Hence, the model is less overfitting and keep the same capacity to generelize unseen data. \\\n",
    "By changing the size of the kernel and focus on one direction, it seems that we made the model less constrain and then, reduced overfitting.\n",
    "\n",
    "Also, I don't really know if it's relevent, but we can look at the running time on the same machine with the same parameters but different kernel sizes (`kernel_size = 5` and `kernel_size = (1,3)`). Using the non-square kernel appears to reduce the running time by 40%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72216181",
   "metadata": {},
   "source": [
    "### 2. Can you find an image that contains neither a bird nor an airplane, but that the model claims has one or the other with more than 95% confidence?\n",
    "\n",
    "* Can you manually edit a neutral image to make it more airplane-like?\n",
    "* Can you manually edit an airplane image to trick the model into reporting a bird?\n",
    "* Do these tasks get easier with a network with less capacity? More capacity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e91bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been successfully loaded in Net(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = save_load(False, True, '', 'databirds_vs_airplanes_kernel=3.pt', Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227436e",
   "metadata": {},
   "source": [
    "Let's construct a `cifar3` variable which is cifar10 without 'airplane' and 'bird' images. Also, since we're interested in probabilities, we'll call the `nn.Softmax` function to convert the output into those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09e1cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "cifar3 = [(img, label) for img, label in cifar10 if label not in [0, 2]]\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "class_names_long = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf60d54",
   "metadata": {},
   "source": [
    "So, we are looking for an image which is neither an airplane or a bird, but that the model claims has one or the other with more than 95% confidence. Because we don't really know where this image is, we'll proceede by random search to avoid computational efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8acf7fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9847\n",
      "The current model found that a 'horse' is actually a 'airplane' with more than 95% of certainty.\n"
     ]
    }
   ],
   "source": [
    "output_prob = torch.tensor(0.)\n",
    "count = 0\n",
    "while torch.max(output_prob).item() <= 0.95:\n",
    "    img, label = rd.choice(cifar3)\n",
    "    output = model(img.unsqueeze(0))\n",
    "    output_prob = softmax(output)\n",
    "    count += 1\n",
    "    if count > len(cifar3):\n",
    "        break\n",
    "    \n",
    "if torch.max(output_prob).item() == output_prob[0][0].item():\n",
    "    prediction = \"airplane\"\n",
    "elif torch.max(output_prob).item() == output_prob[0][1].item():\n",
    "    prediction = \"bird\"\n",
    "\n",
    "print(f\"{torch.max(output_prob).item():.4f}\")\n",
    "print(f\"The current model found that a '{class_names_long[label]}' is actually a '{prediction}' \\\n",
    "with more than 95% of certainty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd911c8",
   "metadata": {},
   "source": [
    "The three last questions are a bit odd to me. So starting here, I will use my common sense and try some things that could lead to the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ad31d",
   "metadata": {},
   "source": [
    "We are looking for a neutral image. So maybe an image that is neither an airplane nor a bird for sure. So, since our model is only capable to identify airplane or bird, we are looking for an image that our model claims to be one or the other with a probability around 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "481c7fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current model found that a 'frog' is actually a 'bird' with a probability of 0.5053.\n"
     ]
    }
   ],
   "source": [
    "output_prob = torch.tensor([[99., 99.]])\n",
    "count = 0\n",
    "\n",
    "while torch.max(output_prob).item() >= 0.55:\n",
    "    img, label = rd.choice(cifar3)\n",
    "    output = model(img.unsqueeze(0))\n",
    "    output_prob = softmax(output)\n",
    "    count += 1\n",
    "    if count > len(cifar3):\n",
    "        print(\"Break time\")\n",
    "        break\n",
    "    \n",
    "if torch.max(output_prob).item() == output_prob[0][0].item():\n",
    "    prediction = \"airplane\"\n",
    "elif torch.max(output_prob).item() == output_prob[0][1].item():\n",
    "    prediction = \"bird\"\n",
    "\n",
    "print(f\"The current model found that a '{class_names_long[label]}' is actually a '{prediction}' \\\n",
    "with a probability of {torch.max(output_prob).item():.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f85e52",
   "metadata": {},
   "source": [
    "We're going to apply some transformation and see if anything is changing regarding the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ceae7762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0025, 0.9975]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe6ade6de50>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms.functional as FT\n",
    "img_w = torch.clone(img)\n",
    "img_wr = FT.rotate(img_w, 115)\n",
    "\n",
    "output = model(img_wr.unsqueeze(0))\n",
    "output_prob = softmax(output)\n",
    "print(output_prob)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img_wr.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03ef31",
   "metadata": {},
   "source": [
    "By rotating the input image by 115 degres, it appears to be more \"airplane-like\" to the model with a probabilty of 99.75%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eba00e",
   "metadata": {},
   "source": [
    "Let's now find an airplane image and try to trick the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2c4d8f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current model found that a 'airplane' is actually a 'airplane' with a probability of 0.9996.\n"
     ]
    }
   ],
   "source": [
    "output_prob = torch.tensor([[0., 0.]])\n",
    "count = 0\n",
    "\n",
    "while torch.max(output_prob).item() <= 0.98:\n",
    "    img, label = rd.choice(cifar2)\n",
    "    output = model(img.unsqueeze(0))\n",
    "    output_prob = softmax(output)\n",
    "    count += 1\n",
    "    if count > len(cifar2):\n",
    "        print(\"Break time\")\n",
    "        break\n",
    "    \n",
    "if torch.max(output_prob).item() == output_prob[0][0].item():\n",
    "    prediction = \"airplane\"\n",
    "elif torch.max(output_prob).item() == output_prob[0][1].item():\n",
    "    prediction = \"bird\"\n",
    "\n",
    "print(f\"The current model found that a '{class_names[label]}' is actually a '{prediction}' \\\n",
    "with a probability of {torch.max(output_prob).item():.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9387c1ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5058, 0.4942]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_airplane_bird = torch.clone(img)\n",
    "transformer = transforms.RandomErasing(p=1, scale=(0., 1), ratio=(1, 1), value=0, inplace=False)\n",
    "img_airplane_bird = transformer(img_airplane_bird)\n",
    "\n",
    "output = model(img_airplane_bird.unsqueeze(0))\n",
    "output_prob = softmax(output)\n",
    "print(output_prob)\n",
    "plt.imshow(img_airplane_bird.permute(1,2,0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728500f",
   "metadata": {},
   "source": [
    "Against all wishes, it's actually very complicated to trick this simple model. Even by erasing a large part of the image, it still convinced that it is a plane. Only with luck, when the crop is in a particular area, we reach 50% of probability. At the contrary, if we rotate the image like the one before, there is a switch with a 120 degres rotation. \n",
    "\n",
    "Does that mean that our model is not rotation invariant and that it doesn't keep tracks of the shapes when they are rotate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "961ca283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1668, 0.8332]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_airplane_bird2 = torch.clone(img)\n",
    "img_wr = FT.rotate(img_airplane_bird2, 120)\n",
    "\n",
    "output = model(img_wr.unsqueeze(0))\n",
    "output_prob = softmax(output)\n",
    "print(output_prob)\n",
    "plt.imshow(img_wr.permute(1,2,0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8a832",
   "metadata": {},
   "source": [
    "* Do these tasks get easier with a network with less capacity? More capacity? \\\n",
    "I do not know how to conclude on this question... Maybe one has to think about overfitting and underfitting.\n",
    "To be continued... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5b384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
