{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb1da27",
   "metadata": {},
   "source": [
    "# Chapter 7 : Telling birds from airplanes: Learning from images\n",
    "\n",
    "### 1. Use torchvision to implement random cropping of the data.\n",
    "* How are the resulting images different from the uncropped originals?\n",
    "* What happens when you request the same image a second time?\n",
    "* What is the result of training using randomly cropped images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa4dcd",
   "metadata": {},
   "source": [
    "In this exercise we'll use the torchvision framework to train a model on a dataset called CIFAR10 which is provided by torchvision.\\\n",
    "First, we'll recall some example of the book : how to load CIFAR10, how images are labelized, how to use the *transforms* module... \\\n",
    "Then, we'll train a model on that dataset in regard to the exercise by considering a *classification problem*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c928339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: data\n",
       "     Split: Train,\n",
       " Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: data\n",
       "     Split: Test)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "data_path = 'data'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False) \n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=False)\n",
    "\n",
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "cifar10, cifar10_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6178cb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img, label = cifar10[2001] # arbitrary image\n",
    "img, label, class_names[label]\n",
    "\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c56004e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7cdad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(121)\n",
    "cropping = transforms.RandomCrop(20)\n",
    "plt.imshow(cropping(img))\n",
    "\n",
    "plt.subplot(122)\n",
    "cropping = transforms.RandomCrop(20)\n",
    "plt.imshow(cropping(img));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c131f",
   "metadata": {},
   "source": [
    "As expected, the resulting image is cropped top the size mentioned in the *RandomCrop* method. Since it's random, the crop isn't taken at the same location twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7fc069",
   "metadata": {},
   "source": [
    "Now, let's build our dataset and train or model on it. We'll have to use Tensor instead of PIL images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e1321",
   "metadata": {},
   "source": [
    "First, we transform the CIFAR10 dataset into Tensor to find the mean/std to normalized the dataset so that each channel has zero mean and unitary standard deviation. In the book, we can find the reasons why : \"*keeping the data in the same range means it’s more likely that neurons have nonzero gradients and, hence, will learn sooner. Also, normalizing each channel so that it has the same distribution will ensure that channel information can be mixed and updated through gradient descent using the same learning rate*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5046dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20, 20, 50000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.RandomCrop(20)]))\n",
    "imgs = torch.stack([img for img, _ in cifar10], dim=3)\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aea9fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20000000])\n",
      "mean :  tensor([0.4905, 0.4708, 0.4348]) \n",
      "std :  tensor([0.2402, 0.2362, 0.2510])\n"
     ]
    }
   ],
   "source": [
    "# Keeping the 3 channels and merging all the remaining dimensions into one\n",
    "print(imgs.view(3,-1).shape)\n",
    "\n",
    "# Finding the mean and std of the stack for normalization\n",
    "mean = imgs.view(3,-1).mean(dim=1)\n",
    "std = imgs.view(3,-1).std(dim=1)\n",
    "print(\"mean : \", mean, \"\\nstd : \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09a0a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.RandomCrop(20),\n",
    "                                 transforms.Normalize((0.4903, 0.4707, 0.4347), (0.2402, 0.2361, 0.2507))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dbbc223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               RandomCrop(size=(20, 20), padding=None)\n",
       "               Normalize(mean=(0.4903, 0.4707, 0.4347), std=(0.2402, 0.2361, 0.2507))\n",
       "           )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                          transform=preprocessor)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=False,\n",
    "                          transform=preprocessor)\n",
    "cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a657054",
   "metadata": {},
   "source": [
    "Since the crop is random, the mean and std of the stack will be slightly different each time so should be the normalization. Because this difference is small we'll stick to this normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22671f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img1, _ = cifar10[2001]\n",
    "img2, _ = cifar10[2001]\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1.permute(1, 2, 0))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2.permute(1, 2, 0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965791b6",
   "metadata": {},
   "source": [
    "Surprisingly, the preprocessor seems to stick to each sample ! Each time we call the cifar10 variable, it calls the *transforms.Compose* preprocessing and a random cropping is execute too !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32aea2",
   "metadata": {},
   "source": [
    "We recall the goal of this exercise : telling birds from airplanes. To work on different data, let's modify this by *telling trucks from ships*. So let's filter the data in CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7ef7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {8: 0, 9: 1}\n",
    "class_names = ['ship', 'truck']\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [8, 9]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [8, 9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5c3f9",
   "metadata": {},
   "source": [
    "Each image has a size of 20x20 after cropping. And each one has 3 channel for the RGB colors. So the input features are 3x20x20 = 1200. \\\n",
    "Also, we need to interpret our output in terms of probabilities (it's either a truck or a ship), so we introduce the Softmax function : \"*Softmax is a monotone function, in that lower values in the input will correspond to lower values in the output. However, it’s not scale invariant, in that the ratio between values is not preserved*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1179f7c",
   "metadata": {},
   "source": [
    "Here, we can chose *nn.LogSoftmax* instead to be able to use the *negative log likelihood nn.NLLLoss* as our loss (see Chapter 7.2.5). And to be more Pytorch-like, we'll go with *nn.CrossEntropyLoss* which is the combination of using *nn.LogSoftmax* and *nn.NLLLoss* as it is explained in the end of the 7.2.6 section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5361717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initilization\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "n_out = 2\n",
    "model = nn.Sequential(nn.Linear(1200, 512),\n",
    "                     nn.Tanh(),\n",
    "                     nn.Linear(512,n_out))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e84c0",
   "metadata": {},
   "source": [
    "Now we'd like to evaluate all the 10000 images, except it would be too much in a single batch and it may conducts the optimization process to get stuck in local minima. By shuffling samples at each epoch and estimating the gradient on a few samples at a time, we are introducing randomness in our gradient descent which helps convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bf91d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.528837\n",
      "Epoch: 1, Training Loss: 0.578594\n",
      "Epoch: 2, Training Loss: 0.429819\n",
      "Epoch: 10, Training Loss: 0.274383\n",
      "Epoch: 20, Training Loss: 0.543682\n",
      "Epoch: 30, Training Loss: 0.256672\n",
      "Epoch: 40, Training Loss: 0.514291\n",
      "Epoch: 50, Training Loss: 0.391341\n",
      "Epoch: 60, Training Loss: 0.166963\n",
      "Epoch: 70, Training Loss: 0.036834\n",
      "Epoch: 80, Training Loss: 0.123754\n",
      "Epoch: 90, Training Loss: 0.070163\n",
      "Execution took: 0:02:04 secs (Wall clock time)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def training_loop(nb_epochs, model, optimizer, loss_fn, train_dataloader):\n",
    "    for epoch in range(nb_epochs):\n",
    "        for imgs, labels in train_dataloader:\n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = model(imgs.view(batch_size, -1)) # resulting in a (64x1200) Tensor\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch == 2 or epoch %10 == 0:\n",
    "            print(\"Epoch: %d, Training Loss: %f\" % (epoch, float(loss)))\n",
    "    \n",
    "\n",
    "training_loop(100, model, optimizer, loss_fn, train_loader)\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a25f6",
   "metadata": {},
   "source": [
    "Well, it's a good start : our model seems to provide a good fit on the train data. \\\n",
    "Let's compute the accuracy which is a more intuitive way to deal with classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cd3f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9946\n"
     ]
    }
   ],
   "source": [
    "### Train data accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1)) # prediction\n",
    "        _, predicted = torch.max(outputs, dim=1) \n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29294dc",
   "metadata": {},
   "source": [
    "By using cropped images, the model seems to train correctly on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9efc1cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.752\n"
     ]
    }
   ],
   "source": [
    "### Validation data accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1)) # prediction\n",
    "        _, predicted = torch.max(outputs, dim=1) \n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0eeea0",
   "metadata": {},
   "source": [
    "On the contrary, the accuracy on unseen data is not satisfying : our model clearly overfits since it captures well the complexity of the training set but can't generalize enough on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786eacd8",
   "metadata": {},
   "source": [
    "### 2. Switch loss functions (perhaps MSE).\n",
    "* Does the training behavior change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21a15fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop2(nb_epochs, model, optimizer, loss_fn, train_dataloader):\n",
    "    for epoch in range(nb_epochs):\n",
    "        for imgs, labels in train_dataloader:\n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = model(imgs.view(batch_size, -1)) # resulting in a (64x1200) Tensor\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(predicted.to(torch.float32), labels)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch == 2 or epoch %10 == 0:\n",
    "            print(\"Epoch: %d, Training Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a24ef3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution took: 0:00:00 secs (Wall clock time)\n"
     ]
    }
   ],
   "source": [
    "n_out = 2\n",
    "model = nn.Sequential(nn.Linear(1200, 512),\n",
    "                     nn.Tanh(),\n",
    "                     nn.Linear(512, n_out))\n",
    "\n",
    "loss_fn2 = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# training_loop2(100, model, optimizer, loss_fn2, train_loader)\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e5ee56",
   "metadata": {},
   "source": [
    "This question need further interest since I can't run the training loop with MSEloss function in the case of categorial problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d19e4e",
   "metadata": {},
   "source": [
    "### 3. Is it possible to reduce the capacity of the network enough that it stops overfitting? \n",
    "* How does the model perform on the validation set when doing so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fa13ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.475922\n",
      "Epoch: 1, Training Loss: 0.464955\n",
      "Epoch: 2, Training Loss: 0.646715\n",
      "Epoch: 10, Training Loss: 0.606024\n",
      "Epoch: 20, Training Loss: 0.844688\n",
      "Epoch: 30, Training Loss: 0.459089\n",
      "Epoch: 40, Training Loss: 0.399365\n",
      "Epoch: 50, Training Loss: 0.380740\n",
      "Epoch: 60, Training Loss: 0.317419\n",
      "Epoch: 70, Training Loss: 0.411381\n",
      "Epoch: 80, Training Loss: 0.403583\n",
      "Epoch: 90, Training Loss: 0.529729\n",
      "Epoch: 100, Training Loss: 0.360211\n",
      "Execution took: 0:00:15 secs (Wall clock time)\n"
     ]
    }
   ],
   "source": [
    "n_out = 2\n",
    "model = nn.Sequential(nn.Linear(1200, 2),\n",
    "                     nn.Tanh(),\n",
    "                     nn.Linear(2, n_out))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "training_loop(101, model, optimizer, loss_fn, train_loader)\n",
    "\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "msg = \"Execution took: %s secs (Wall clock time)\" % timedelta(seconds=round(elapsed_time_secs))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfe91405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7794\n",
      "Validation Accuracy: 0.6965\n"
     ]
    }
   ],
   "source": [
    "### Train data accuracy\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1)) # prediction\n",
    "        _, predicted = torch.max(outputs, dim=1) \n",
    "        total_train += labels.shape[0]\n",
    "        correct_train += int((predicted == labels).sum())\n",
    "\n",
    "### Validation data accuracy\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1)) # prediction\n",
    "        _, predicted = torch.max(outputs, dim=1) \n",
    "        total_val += labels.shape[0]\n",
    "        correct_val += int((predicted == labels).sum())\n",
    "\n",
    "\n",
    "print(\"Train Accuracy:\", correct_train / total_train)\n",
    "print(\"Validation Accuracy:\", correct_val / total_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bb4d8",
   "metadata": {},
   "source": [
    "By drastically reducing the size of the hidden layer, we create a model less constrain and more likely to underfit the data. Since the train accuracy and the validation accuracy are closer together, the model less overfits the data. In return, it leads to a small decrease of validation accuracy (with a deeper network, it is possible to go further with an accuracy of 0.8020 on the validation set. See 7.2.5). \\\n",
    "What is important is the validation accuracy. By reducing the model complexity, we obtain almost the same accuracy result (compared to our former model, not the complexe one of the book) but especially, we decreased widely the computation cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
